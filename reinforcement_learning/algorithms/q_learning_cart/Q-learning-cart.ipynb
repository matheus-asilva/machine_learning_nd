{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep $Q$-learning\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use $Q$-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://github.com/openai/gym). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Number of possible actions: 2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Number of possible actions\n",
    "print('Number of possible actions:', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`.  You can see how many actions are possible from `env.action_space.n`, and to get a random action you can use `env.action_space.sample()`.  Passing in an action as an integer to `env.step` will generate the next step in the simulation.  This is general to all Gym games. \n",
    "\n",
    "In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [] # actions that the agent selects\n",
    "rewards = [] # obtained rewards\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    action = env.action_space.sample()  # choose a random action\n",
    "    state, reward, done, _ = env.step(action) \n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the actions and rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: [0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "Rewards: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print('Actions:', actions)\n",
    "print('Rewards:', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions: [0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n",
    "Rewards: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each step while the game is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right.\n",
    "\n",
    "## $Q$-Network\n",
    "\n",
    "To keep track of the action values, we'll use a neural network that accepts a state $s$ as input.  The output will be $Q$-values for each available action $a$ (i.e., the output is **all** action values $Q(s,a)$ _corresponding to the input state $s$_).\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "For this Cart-Pole game, the state has four values: the position and velocity of the cart, and the position and velocity of the pole.  Thus, the neural network has **four inputs**, one for each value in the state, and **two outputs**, one for each possible action. \n",
    "\n",
    "As explored in the lesson, to get the training target, we'll first use the context provided by the state $s$ to choose an action $a$, then simulate the game using that action. This will get us the next state, $s'$, and the reward $r$. With that, we can calculate $\\hat{Q}(s,a) = r + \\gamma \\max_{a'}{Q(s', a')}$.  Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "Below is one implementation of the $Q$-network. It uses two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class QNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=4, \n",
    "                 action_size=2, hidden_size=10, \n",
    "                 name='QNetwork'):\n",
    "        # state inputs to the Q-network\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            \n",
    "            # One hot encode the actions to later choose the Q-value for the action\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "            \n",
    "            # Target Q values for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')\n",
    "            \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size)\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size)\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(self.fc2, action_size, \n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # output has length 2, for two actions. This next line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maximum capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Q$-Learning training algorithm\n",
    "\n",
    "We will use the below algorithm to train the network.  For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode $\\leftarrow 1$ **to** $M$ **do**\n",
    "  * Observe $s_0$\n",
    "  * **For** $t \\leftarrow 0$ **to** $T-1$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**\n",
    "\n",
    "You are welcome (and encouraged!) to take the time to extend this code to implement some of the improvements that we discussed in the lesson, to include fixed $Q$ targets, double DQNs, prioritized replay, and/or dueling networks.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcement learning is the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here we re-initialize the simulation and pre-populate the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for ii in range(pretrain_length):\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total reward: 2.0 Training loss: 1.0699 Explore P: 0.9998\n",
      "Episode: 2 Total reward: 22.0 Training loss: 1.0614 Explore P: 0.9976\n",
      "Episode: 3 Total reward: 17.0 Training loss: 1.0420 Explore P: 0.9959\n",
      "Episode: 4 Total reward: 16.0 Training loss: 1.0396 Explore P: 0.9944\n",
      "Episode: 5 Total reward: 15.0 Training loss: 1.0226 Explore P: 0.9929\n",
      "Episode: 6 Total reward: 10.0 Training loss: 1.0071 Explore P: 0.9919\n",
      "Episode: 7 Total reward: 10.0 Training loss: 1.0922 Explore P: 0.9909\n",
      "Episode: 8 Total reward: 45.0 Training loss: 1.0304 Explore P: 0.9865\n",
      "Episode: 9 Total reward: 11.0 Training loss: 1.0812 Explore P: 0.9855\n",
      "Episode: 10 Total reward: 14.0 Training loss: 1.0358 Explore P: 0.9841\n",
      "Episode: 11 Total reward: 17.0 Training loss: 1.1156 Explore P: 0.9824\n",
      "Episode: 12 Total reward: 26.0 Training loss: 1.0695 Explore P: 0.9799\n",
      "Episode: 13 Total reward: 25.0 Training loss: 1.0227 Explore P: 0.9775\n",
      "Episode: 14 Total reward: 16.0 Training loss: 1.0049 Explore P: 0.9759\n",
      "Episode: 15 Total reward: 19.0 Training loss: 0.9299 Explore P: 0.9741\n",
      "Episode: 16 Total reward: 12.0 Training loss: 1.0274 Explore P: 0.9730\n",
      "Episode: 17 Total reward: 17.0 Training loss: 1.1306 Explore P: 0.9713\n",
      "Episode: 18 Total reward: 13.0 Training loss: 1.1915 Explore P: 0.9701\n",
      "Episode: 19 Total reward: 17.0 Training loss: 1.2186 Explore P: 0.9684\n",
      "Episode: 20 Total reward: 27.0 Training loss: 1.4692 Explore P: 0.9659\n",
      "Episode: 21 Total reward: 14.0 Training loss: 1.2778 Explore P: 0.9645\n",
      "Episode: 22 Total reward: 21.0 Training loss: 1.5378 Explore P: 0.9625\n",
      "Episode: 23 Total reward: 14.0 Training loss: 1.2793 Explore P: 0.9612\n",
      "Episode: 24 Total reward: 15.0 Training loss: 1.2809 Explore P: 0.9598\n",
      "Episode: 25 Total reward: 105.0 Training loss: 1.8452 Explore P: 0.9498\n",
      "Episode: 26 Total reward: 9.0 Training loss: 1.6511 Explore P: 0.9490\n",
      "Episode: 27 Total reward: 24.0 Training loss: 1.9580 Explore P: 0.9467\n",
      "Episode: 28 Total reward: 50.0 Training loss: 1.8995 Explore P: 0.9421\n",
      "Episode: 29 Total reward: 22.0 Training loss: 2.7454 Explore P: 0.9400\n",
      "Episode: 30 Total reward: 12.0 Training loss: 3.4225 Explore P: 0.9389\n",
      "Episode: 31 Total reward: 21.0 Training loss: 2.4530 Explore P: 0.9370\n",
      "Episode: 32 Total reward: 18.0 Training loss: 3.3600 Explore P: 0.9353\n",
      "Episode: 33 Total reward: 52.0 Training loss: 3.9488 Explore P: 0.9305\n",
      "Episode: 34 Total reward: 41.0 Training loss: 3.7567 Explore P: 0.9267\n",
      "Episode: 35 Total reward: 19.0 Training loss: 6.3708 Explore P: 0.9250\n",
      "Episode: 36 Total reward: 14.0 Training loss: 3.1444 Explore P: 0.9237\n",
      "Episode: 37 Total reward: 10.0 Training loss: 3.4011 Explore P: 0.9228\n",
      "Episode: 38 Total reward: 13.0 Training loss: 3.2434 Explore P: 0.9216\n",
      "Episode: 39 Total reward: 11.0 Training loss: 14.2622 Explore P: 0.9206\n",
      "Episode: 40 Total reward: 12.0 Training loss: 2.9522 Explore P: 0.9195\n",
      "Episode: 41 Total reward: 31.0 Training loss: 5.5049 Explore P: 0.9167\n",
      "Episode: 42 Total reward: 13.0 Training loss: 3.7781 Explore P: 0.9155\n",
      "Episode: 43 Total reward: 55.0 Training loss: 5.6387 Explore P: 0.9105\n",
      "Episode: 44 Total reward: 11.0 Training loss: 3.8148 Explore P: 0.9096\n",
      "Episode: 45 Total reward: 22.0 Training loss: 3.3088 Explore P: 0.9076\n",
      "Episode: 46 Total reward: 9.0 Training loss: 6.6396 Explore P: 0.9068\n",
      "Episode: 47 Total reward: 17.0 Training loss: 8.9669 Explore P: 0.9053\n",
      "Episode: 48 Total reward: 17.0 Training loss: 5.0153 Explore P: 0.9037\n",
      "Episode: 49 Total reward: 20.0 Training loss: 9.7949 Explore P: 0.9019\n",
      "Episode: 50 Total reward: 24.0 Training loss: 15.3325 Explore P: 0.8998\n",
      "Episode: 51 Total reward: 69.0 Training loss: 18.7135 Explore P: 0.8937\n",
      "Episode: 52 Total reward: 25.0 Training loss: 58.8857 Explore P: 0.8915\n",
      "Episode: 53 Total reward: 19.0 Training loss: 10.2070 Explore P: 0.8898\n",
      "Episode: 54 Total reward: 26.0 Training loss: 16.2683 Explore P: 0.8875\n",
      "Episode: 55 Total reward: 16.0 Training loss: 27.8030 Explore P: 0.8861\n",
      "Episode: 56 Total reward: 28.0 Training loss: 6.4296 Explore P: 0.8837\n",
      "Episode: 57 Total reward: 17.0 Training loss: 5.5587 Explore P: 0.8822\n",
      "Episode: 58 Total reward: 15.0 Training loss: 7.7065 Explore P: 0.8809\n",
      "Episode: 59 Total reward: 23.0 Training loss: 5.8398 Explore P: 0.8789\n",
      "Episode: 60 Total reward: 12.0 Training loss: 6.1992 Explore P: 0.8778\n",
      "Episode: 61 Total reward: 17.0 Training loss: 5.3996 Explore P: 0.8764\n",
      "Episode: 62 Total reward: 17.0 Training loss: 37.1076 Explore P: 0.8749\n",
      "Episode: 63 Total reward: 27.0 Training loss: 44.1580 Explore P: 0.8726\n",
      "Episode: 64 Total reward: 25.0 Training loss: 6.9530 Explore P: 0.8704\n",
      "Episode: 65 Total reward: 9.0 Training loss: 56.2465 Explore P: 0.8696\n",
      "Episode: 66 Total reward: 24.0 Training loss: 46.2304 Explore P: 0.8676\n",
      "Episode: 67 Total reward: 10.0 Training loss: 7.4581 Explore P: 0.8667\n",
      "Episode: 68 Total reward: 15.0 Training loss: 5.8934 Explore P: 0.8654\n",
      "Episode: 69 Total reward: 25.0 Training loss: 7.1556 Explore P: 0.8633\n",
      "Episode: 70 Total reward: 11.0 Training loss: 26.0204 Explore P: 0.8624\n",
      "Episode: 71 Total reward: 16.0 Training loss: 122.6330 Explore P: 0.8610\n",
      "Episode: 72 Total reward: 44.0 Training loss: 93.4734 Explore P: 0.8573\n",
      "Episode: 73 Total reward: 10.0 Training loss: 8.5134 Explore P: 0.8564\n",
      "Episode: 74 Total reward: 11.0 Training loss: 70.2444 Explore P: 0.8555\n",
      "Episode: 75 Total reward: 26.0 Training loss: 11.9210 Explore P: 0.8533\n",
      "Episode: 76 Total reward: 10.0 Training loss: 37.5854 Explore P: 0.8524\n",
      "Episode: 77 Total reward: 20.0 Training loss: 9.1491 Explore P: 0.8508\n",
      "Episode: 78 Total reward: 14.0 Training loss: 102.5106 Explore P: 0.8496\n",
      "Episode: 79 Total reward: 18.0 Training loss: 9.2250 Explore P: 0.8481\n",
      "Episode: 80 Total reward: 9.0 Training loss: 10.5408 Explore P: 0.8473\n",
      "Episode: 81 Total reward: 47.0 Training loss: 101.1814 Explore P: 0.8434\n",
      "Episode: 82 Total reward: 14.0 Training loss: 35.8844 Explore P: 0.8422\n",
      "Episode: 83 Total reward: 41.0 Training loss: 7.8602 Explore P: 0.8388\n",
      "Episode: 84 Total reward: 26.0 Training loss: 66.6510 Explore P: 0.8367\n",
      "Episode: 85 Total reward: 30.0 Training loss: 123.7617 Explore P: 0.8342\n",
      "Episode: 86 Total reward: 9.0 Training loss: 7.1262 Explore P: 0.8335\n",
      "Episode: 87 Total reward: 21.0 Training loss: 6.6918 Explore P: 0.8317\n",
      "Episode: 88 Total reward: 17.0 Training loss: 7.4691 Explore P: 0.8303\n",
      "Episode: 89 Total reward: 18.0 Training loss: 59.3176 Explore P: 0.8289\n",
      "Episode: 90 Total reward: 13.0 Training loss: 119.2395 Explore P: 0.8278\n",
      "Episode: 91 Total reward: 8.0 Training loss: 9.1038 Explore P: 0.8271\n",
      "Episode: 92 Total reward: 13.0 Training loss: 76.9910 Explore P: 0.8261\n",
      "Episode: 93 Total reward: 11.0 Training loss: 8.2275 Explore P: 0.8252\n",
      "Episode: 94 Total reward: 15.0 Training loss: 104.3211 Explore P: 0.8240\n",
      "Episode: 95 Total reward: 7.0 Training loss: 76.8598 Explore P: 0.8234\n",
      "Episode: 96 Total reward: 79.0 Training loss: 53.7604 Explore P: 0.8170\n",
      "Episode: 97 Total reward: 12.0 Training loss: 112.4376 Explore P: 0.8160\n",
      "Episode: 98 Total reward: 17.0 Training loss: 6.0946 Explore P: 0.8146\n",
      "Episode: 99 Total reward: 7.0 Training loss: 5.5496 Explore P: 0.8141\n",
      "Episode: 100 Total reward: 27.0 Training loss: 75.4647 Explore P: 0.8119\n",
      "Episode: 101 Total reward: 23.0 Training loss: 4.0012 Explore P: 0.8101\n",
      "Episode: 102 Total reward: 26.0 Training loss: 3.4038 Explore P: 0.8080\n",
      "Episode: 103 Total reward: 30.0 Training loss: 133.7614 Explore P: 0.8056\n",
      "Episode: 104 Total reward: 15.0 Training loss: 218.0490 Explore P: 0.8044\n",
      "Episode: 105 Total reward: 23.0 Training loss: 4.3237 Explore P: 0.8026\n",
      "Episode: 106 Total reward: 14.0 Training loss: 63.0406 Explore P: 0.8015\n",
      "Episode: 107 Total reward: 13.0 Training loss: 60.0202 Explore P: 0.8005\n",
      "Episode: 108 Total reward: 28.0 Training loss: 5.9899 Explore P: 0.7982\n",
      "Episode: 109 Total reward: 19.0 Training loss: 129.4603 Explore P: 0.7967\n",
      "Episode: 110 Total reward: 14.0 Training loss: 49.6633 Explore P: 0.7956\n",
      "Episode: 111 Total reward: 15.0 Training loss: 48.2624 Explore P: 0.7945\n",
      "Episode: 112 Total reward: 30.0 Training loss: 77.2181 Explore P: 0.7921\n",
      "Episode: 113 Total reward: 11.0 Training loss: 67.2419 Explore P: 0.7913\n",
      "Episode: 114 Total reward: 10.0 Training loss: 49.6400 Explore P: 0.7905\n",
      "Episode: 115 Total reward: 9.0 Training loss: 64.2648 Explore P: 0.7898\n",
      "Episode: 116 Total reward: 17.0 Training loss: 3.7062 Explore P: 0.7885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 117 Total reward: 29.0 Training loss: 79.3176 Explore P: 0.7862\n",
      "Episode: 118 Total reward: 18.0 Training loss: 1.9413 Explore P: 0.7848\n",
      "Episode: 119 Total reward: 12.0 Training loss: 219.5486 Explore P: 0.7839\n",
      "Episode: 120 Total reward: 19.0 Training loss: 111.5701 Explore P: 0.7824\n",
      "Episode: 121 Total reward: 12.0 Training loss: 82.9906 Explore P: 0.7815\n",
      "Episode: 122 Total reward: 15.0 Training loss: 170.6405 Explore P: 0.7803\n",
      "Episode: 123 Total reward: 11.0 Training loss: 5.0764 Explore P: 0.7795\n",
      "Episode: 124 Total reward: 22.0 Training loss: 122.5777 Explore P: 0.7778\n",
      "Episode: 125 Total reward: 12.0 Training loss: 57.5698 Explore P: 0.7769\n",
      "Episode: 126 Total reward: 12.0 Training loss: 1.9285 Explore P: 0.7759\n",
      "Episode: 127 Total reward: 11.0 Training loss: 237.6936 Explore P: 0.7751\n",
      "Episode: 128 Total reward: 9.0 Training loss: 119.2509 Explore P: 0.7744\n",
      "Episode: 129 Total reward: 19.0 Training loss: 59.6291 Explore P: 0.7730\n",
      "Episode: 130 Total reward: 11.0 Training loss: 4.2427 Explore P: 0.7721\n",
      "Episode: 131 Total reward: 15.0 Training loss: 48.9612 Explore P: 0.7710\n",
      "Episode: 132 Total reward: 27.0 Training loss: 1.7380 Explore P: 0.7689\n",
      "Episode: 133 Total reward: 12.0 Training loss: 85.6066 Explore P: 0.7680\n",
      "Episode: 134 Total reward: 9.0 Training loss: 2.8952 Explore P: 0.7673\n",
      "Episode: 135 Total reward: 11.0 Training loss: 52.1415 Explore P: 0.7665\n",
      "Episode: 136 Total reward: 13.0 Training loss: 102.6549 Explore P: 0.7655\n",
      "Episode: 137 Total reward: 13.0 Training loss: 111.3398 Explore P: 0.7645\n",
      "Episode: 138 Total reward: 11.0 Training loss: 99.5999 Explore P: 0.7637\n",
      "Episode: 139 Total reward: 18.0 Training loss: 58.5132 Explore P: 0.7624\n",
      "Episode: 140 Total reward: 10.0 Training loss: 2.3345 Explore P: 0.7616\n",
      "Episode: 141 Total reward: 14.0 Training loss: 51.9142 Explore P: 0.7605\n",
      "Episode: 142 Total reward: 9.0 Training loss: 41.5673 Explore P: 0.7599\n",
      "Episode: 143 Total reward: 9.0 Training loss: 163.0135 Explore P: 0.7592\n",
      "Episode: 144 Total reward: 14.0 Training loss: 2.0518 Explore P: 0.7582\n",
      "Episode: 145 Total reward: 18.0 Training loss: 2.2539 Explore P: 0.7568\n",
      "Episode: 146 Total reward: 12.0 Training loss: 2.1736 Explore P: 0.7559\n",
      "Episode: 147 Total reward: 9.0 Training loss: 77.8664 Explore P: 0.7552\n",
      "Episode: 148 Total reward: 18.0 Training loss: 72.5631 Explore P: 0.7539\n",
      "Episode: 149 Total reward: 16.0 Training loss: 1.7117 Explore P: 0.7527\n",
      "Episode: 150 Total reward: 14.0 Training loss: 0.7369 Explore P: 0.7517\n",
      "Episode: 151 Total reward: 12.0 Training loss: 2.5535 Explore P: 0.7508\n",
      "Episode: 152 Total reward: 25.0 Training loss: 46.5842 Explore P: 0.7489\n",
      "Episode: 153 Total reward: 12.0 Training loss: 91.9474 Explore P: 0.7480\n",
      "Episode: 154 Total reward: 14.0 Training loss: 94.4539 Explore P: 0.7470\n",
      "Episode: 155 Total reward: 11.0 Training loss: 2.7491 Explore P: 0.7462\n",
      "Episode: 156 Total reward: 23.0 Training loss: 44.7843 Explore P: 0.7445\n",
      "Episode: 157 Total reward: 12.0 Training loss: 130.5074 Explore P: 0.7436\n",
      "Episode: 158 Total reward: 15.0 Training loss: 1.5811 Explore P: 0.7425\n",
      "Episode: 159 Total reward: 22.0 Training loss: 96.9203 Explore P: 0.7409\n",
      "Episode: 160 Total reward: 13.0 Training loss: 1.9004 Explore P: 0.7400\n",
      "Episode: 161 Total reward: 14.0 Training loss: 67.1955 Explore P: 0.7389\n",
      "Episode: 162 Total reward: 31.0 Training loss: 176.6270 Explore P: 0.7367\n",
      "Episode: 163 Total reward: 17.0 Training loss: 2.7997 Explore P: 0.7355\n",
      "Episode: 164 Total reward: 11.0 Training loss: 29.7323 Explore P: 0.7347\n",
      "Episode: 165 Total reward: 7.0 Training loss: 59.5104 Explore P: 0.7342\n",
      "Episode: 166 Total reward: 24.0 Training loss: 76.0014 Explore P: 0.7324\n",
      "Episode: 167 Total reward: 26.0 Training loss: 3.2311 Explore P: 0.7305\n",
      "Episode: 168 Total reward: 13.0 Training loss: 128.7064 Explore P: 0.7296\n",
      "Episode: 169 Total reward: 16.0 Training loss: 83.0751 Explore P: 0.7285\n",
      "Episode: 170 Total reward: 12.0 Training loss: 28.1259 Explore P: 0.7276\n",
      "Episode: 171 Total reward: 8.0 Training loss: 69.0928 Explore P: 0.7270\n",
      "Episode: 172 Total reward: 26.0 Training loss: 2.4739 Explore P: 0.7252\n",
      "Episode: 173 Total reward: 26.0 Training loss: 29.8055 Explore P: 0.7233\n",
      "Episode: 174 Total reward: 11.0 Training loss: 32.5163 Explore P: 0.7225\n",
      "Episode: 175 Total reward: 17.0 Training loss: 1.4960 Explore P: 0.7213\n",
      "Episode: 176 Total reward: 25.0 Training loss: 83.4598 Explore P: 0.7195\n",
      "Episode: 177 Total reward: 13.0 Training loss: 62.9673 Explore P: 0.7186\n",
      "Episode: 178 Total reward: 17.0 Training loss: 92.1576 Explore P: 0.7174\n",
      "Episode: 179 Total reward: 10.0 Training loss: 121.0316 Explore P: 0.7167\n",
      "Episode: 180 Total reward: 26.0 Training loss: 132.2867 Explore P: 0.7149\n",
      "Episode: 181 Total reward: 29.0 Training loss: 28.9844 Explore P: 0.7128\n",
      "Episode: 182 Total reward: 14.0 Training loss: 28.0086 Explore P: 0.7118\n",
      "Episode: 183 Total reward: 25.0 Training loss: 65.2408 Explore P: 0.7101\n",
      "Episode: 184 Total reward: 12.0 Training loss: 96.2595 Explore P: 0.7092\n",
      "Episode: 185 Total reward: 12.0 Training loss: 2.6319 Explore P: 0.7084\n",
      "Episode: 186 Total reward: 10.0 Training loss: 31.6182 Explore P: 0.7077\n",
      "Episode: 187 Total reward: 18.0 Training loss: 2.8847 Explore P: 0.7065\n",
      "Episode: 188 Total reward: 16.0 Training loss: 3.5375 Explore P: 0.7053\n",
      "Episode: 189 Total reward: 16.0 Training loss: 54.9863 Explore P: 0.7042\n",
      "Episode: 190 Total reward: 17.0 Training loss: 87.0149 Explore P: 0.7031\n",
      "Episode: 191 Total reward: 31.0 Training loss: 114.5495 Explore P: 0.7009\n",
      "Episode: 192 Total reward: 28.0 Training loss: 58.0686 Explore P: 0.6990\n",
      "Episode: 193 Total reward: 12.0 Training loss: 4.7341 Explore P: 0.6981\n",
      "Episode: 194 Total reward: 12.0 Training loss: 4.5512 Explore P: 0.6973\n",
      "Episode: 195 Total reward: 12.0 Training loss: 117.2704 Explore P: 0.6965\n",
      "Episode: 196 Total reward: 22.0 Training loss: 2.7077 Explore P: 0.6950\n",
      "Episode: 197 Total reward: 21.0 Training loss: 3.3867 Explore P: 0.6936\n",
      "Episode: 198 Total reward: 9.0 Training loss: 71.5425 Explore P: 0.6929\n",
      "Episode: 199 Total reward: 15.0 Training loss: 53.5495 Explore P: 0.6919\n",
      "Episode: 200 Total reward: 37.0 Training loss: 23.1833 Explore P: 0.6894\n",
      "Episode: 201 Total reward: 11.0 Training loss: 68.0909 Explore P: 0.6886\n",
      "Episode: 202 Total reward: 16.0 Training loss: 30.9414 Explore P: 0.6876\n",
      "Episode: 203 Total reward: 10.0 Training loss: 50.0301 Explore P: 0.6869\n",
      "Episode: 204 Total reward: 14.0 Training loss: 29.0000 Explore P: 0.6859\n",
      "Episode: 205 Total reward: 61.0 Training loss: 66.5578 Explore P: 0.6818\n",
      "Episode: 206 Total reward: 18.0 Training loss: 3.0762 Explore P: 0.6806\n",
      "Episode: 207 Total reward: 26.0 Training loss: 27.2015 Explore P: 0.6789\n",
      "Episode: 208 Total reward: 12.0 Training loss: 140.4793 Explore P: 0.6781\n",
      "Episode: 209 Total reward: 12.0 Training loss: 3.1763 Explore P: 0.6773\n",
      "Episode: 210 Total reward: 12.0 Training loss: 2.8580 Explore P: 0.6765\n",
      "Episode: 211 Total reward: 9.0 Training loss: 56.6161 Explore P: 0.6759\n",
      "Episode: 212 Total reward: 13.0 Training loss: 27.9860 Explore P: 0.6750\n",
      "Episode: 213 Total reward: 22.0 Training loss: 35.1077 Explore P: 0.6736\n",
      "Episode: 214 Total reward: 21.0 Training loss: 3.2120 Explore P: 0.6722\n",
      "Episode: 215 Total reward: 13.0 Training loss: 79.8463 Explore P: 0.6713\n",
      "Episode: 216 Total reward: 15.0 Training loss: 23.9786 Explore P: 0.6703\n",
      "Episode: 217 Total reward: 12.0 Training loss: 31.0722 Explore P: 0.6695\n",
      "Episode: 218 Total reward: 11.0 Training loss: 24.4078 Explore P: 0.6688\n",
      "Episode: 219 Total reward: 14.0 Training loss: 3.8024 Explore P: 0.6679\n",
      "Episode: 220 Total reward: 12.0 Training loss: 2.8033 Explore P: 0.6671\n",
      "Episode: 221 Total reward: 10.0 Training loss: 3.5081 Explore P: 0.6664\n",
      "Episode: 222 Total reward: 24.0 Training loss: 59.2221 Explore P: 0.6648\n",
      "Episode: 223 Total reward: 24.0 Training loss: 74.0889 Explore P: 0.6633\n",
      "Episode: 224 Total reward: 9.0 Training loss: 34.6692 Explore P: 0.6627\n",
      "Episode: 225 Total reward: 10.0 Training loss: 65.4079 Explore P: 0.6620\n",
      "Episode: 226 Total reward: 9.0 Training loss: 91.0704 Explore P: 0.6615\n",
      "Episode: 227 Total reward: 31.0 Training loss: 2.8531 Explore P: 0.6594\n",
      "Episode: 228 Total reward: 8.0 Training loss: 74.4297 Explore P: 0.6589\n",
      "Episode: 229 Total reward: 25.0 Training loss: 205.8594 Explore P: 0.6573\n",
      "Episode: 230 Total reward: 16.0 Training loss: 221.9196 Explore P: 0.6563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 231 Total reward: 14.0 Training loss: 59.8939 Explore P: 0.6554\n",
      "Episode: 232 Total reward: 14.0 Training loss: 28.2389 Explore P: 0.6545\n",
      "Episode: 233 Total reward: 15.0 Training loss: 56.8860 Explore P: 0.6535\n",
      "Episode: 234 Total reward: 24.0 Training loss: 62.8056 Explore P: 0.6519\n",
      "Episode: 235 Total reward: 11.0 Training loss: 55.5959 Explore P: 0.6512\n",
      "Episode: 236 Total reward: 13.0 Training loss: 2.1602 Explore P: 0.6504\n",
      "Episode: 237 Total reward: 15.0 Training loss: 79.6792 Explore P: 0.6494\n",
      "Episode: 238 Total reward: 20.0 Training loss: 57.3186 Explore P: 0.6482\n",
      "Episode: 239 Total reward: 17.0 Training loss: 2.1632 Explore P: 0.6471\n",
      "Episode: 240 Total reward: 12.0 Training loss: 22.4429 Explore P: 0.6463\n",
      "Episode: 241 Total reward: 10.0 Training loss: 2.1320 Explore P: 0.6457\n",
      "Episode: 242 Total reward: 9.0 Training loss: 52.0045 Explore P: 0.6451\n",
      "Episode: 243 Total reward: 10.0 Training loss: 1.3325 Explore P: 0.6445\n",
      "Episode: 244 Total reward: 14.0 Training loss: 70.5708 Explore P: 0.6436\n",
      "Episode: 245 Total reward: 12.0 Training loss: 2.4705 Explore P: 0.6428\n",
      "Episode: 246 Total reward: 14.0 Training loss: 1.8340 Explore P: 0.6419\n",
      "Episode: 247 Total reward: 12.0 Training loss: 120.8773 Explore P: 0.6412\n",
      "Episode: 248 Total reward: 12.0 Training loss: 50.9255 Explore P: 0.6404\n",
      "Episode: 249 Total reward: 13.0 Training loss: 51.3815 Explore P: 0.6396\n",
      "Episode: 250 Total reward: 22.0 Training loss: 1.5584 Explore P: 0.6382\n",
      "Episode: 251 Total reward: 10.0 Training loss: 80.8518 Explore P: 0.6376\n",
      "Episode: 252 Total reward: 13.0 Training loss: 19.9820 Explore P: 0.6368\n",
      "Episode: 253 Total reward: 10.0 Training loss: 22.6710 Explore P: 0.6362\n",
      "Episode: 254 Total reward: 30.0 Training loss: 44.3943 Explore P: 0.6343\n",
      "Episode: 255 Total reward: 10.0 Training loss: 2.6425 Explore P: 0.6337\n",
      "Episode: 256 Total reward: 19.0 Training loss: 67.4867 Explore P: 0.6325\n",
      "Episode: 257 Total reward: 11.0 Training loss: 47.9627 Explore P: 0.6318\n",
      "Episode: 258 Total reward: 61.0 Training loss: 40.6182 Explore P: 0.6280\n",
      "Episode: 259 Total reward: 9.0 Training loss: 30.3132 Explore P: 0.6275\n",
      "Episode: 260 Total reward: 14.0 Training loss: 46.1146 Explore P: 0.6266\n",
      "Episode: 261 Total reward: 20.0 Training loss: 48.3848 Explore P: 0.6254\n",
      "Episode: 262 Total reward: 25.0 Training loss: 43.1217 Explore P: 0.6238\n",
      "Episode: 263 Total reward: 13.0 Training loss: 55.4352 Explore P: 0.6230\n",
      "Episode: 264 Total reward: 8.0 Training loss: 30.7243 Explore P: 0.6225\n",
      "Episode: 265 Total reward: 34.0 Training loss: 89.8751 Explore P: 0.6205\n",
      "Episode: 266 Total reward: 8.0 Training loss: 1.8224 Explore P: 0.6200\n",
      "Episode: 267 Total reward: 14.0 Training loss: 37.4741 Explore P: 0.6191\n",
      "Episode: 268 Total reward: 13.0 Training loss: 98.2548 Explore P: 0.6183\n",
      "Episode: 269 Total reward: 7.0 Training loss: 62.6161 Explore P: 0.6179\n",
      "Episode: 270 Total reward: 12.0 Training loss: 1.5880 Explore P: 0.6172\n",
      "Episode: 271 Total reward: 10.0 Training loss: 2.2505 Explore P: 0.6166\n",
      "Episode: 272 Total reward: 18.0 Training loss: 94.5513 Explore P: 0.6155\n",
      "Episode: 273 Total reward: 10.0 Training loss: 44.9761 Explore P: 0.6149\n",
      "Episode: 274 Total reward: 19.0 Training loss: 41.5452 Explore P: 0.6137\n",
      "Episode: 275 Total reward: 15.0 Training loss: 21.6386 Explore P: 0.6128\n",
      "Episode: 276 Total reward: 12.0 Training loss: 2.1565 Explore P: 0.6121\n",
      "Episode: 277 Total reward: 16.0 Training loss: 36.7773 Explore P: 0.6111\n",
      "Episode: 278 Total reward: 14.0 Training loss: 26.3197 Explore P: 0.6103\n",
      "Episode: 279 Total reward: 18.0 Training loss: 22.8738 Explore P: 0.6092\n",
      "Episode: 280 Total reward: 37.0 Training loss: 19.1639 Explore P: 0.6070\n",
      "Episode: 281 Total reward: 11.0 Training loss: 1.9231 Explore P: 0.6063\n",
      "Episode: 282 Total reward: 24.0 Training loss: 24.2394 Explore P: 0.6049\n",
      "Episode: 283 Total reward: 17.0 Training loss: 1.6092 Explore P: 0.6039\n",
      "Episode: 284 Total reward: 21.0 Training loss: 1.6885 Explore P: 0.6027\n",
      "Episode: 285 Total reward: 24.0 Training loss: 32.3975 Explore P: 0.6012\n",
      "Episode: 286 Total reward: 12.0 Training loss: 1.5192 Explore P: 0.6005\n",
      "Episode: 287 Total reward: 9.0 Training loss: 33.6531 Explore P: 0.6000\n",
      "Episode: 288 Total reward: 19.0 Training loss: 82.1075 Explore P: 0.5989\n",
      "Episode: 289 Total reward: 27.0 Training loss: 34.1888 Explore P: 0.5973\n",
      "Episode: 290 Total reward: 12.0 Training loss: 59.8704 Explore P: 0.5966\n",
      "Episode: 291 Total reward: 17.0 Training loss: 57.1959 Explore P: 0.5956\n",
      "Episode: 292 Total reward: 12.0 Training loss: 27.9036 Explore P: 0.5949\n",
      "Episode: 293 Total reward: 27.0 Training loss: 49.0889 Explore P: 0.5933\n",
      "Episode: 294 Total reward: 16.0 Training loss: 0.8185 Explore P: 0.5924\n",
      "Episode: 295 Total reward: 21.0 Training loss: 0.7641 Explore P: 0.5911\n",
      "Episode: 296 Total reward: 14.0 Training loss: 27.6603 Explore P: 0.5903\n",
      "Episode: 297 Total reward: 13.0 Training loss: 20.4131 Explore P: 0.5896\n",
      "Episode: 298 Total reward: 21.0 Training loss: 0.2660 Explore P: 0.5884\n",
      "Episode: 299 Total reward: 20.0 Training loss: 24.2299 Explore P: 0.5872\n",
      "Episode: 300 Total reward: 17.0 Training loss: 0.7702 Explore P: 0.5862\n",
      "Episode: 301 Total reward: 11.0 Training loss: 0.5944 Explore P: 0.5856\n",
      "Episode: 302 Total reward: 14.0 Training loss: 35.6976 Explore P: 0.5848\n",
      "Episode: 303 Total reward: 9.0 Training loss: 25.9616 Explore P: 0.5843\n",
      "Episode: 304 Total reward: 17.0 Training loss: 47.3333 Explore P: 0.5833\n",
      "Episode: 305 Total reward: 10.0 Training loss: 25.2361 Explore P: 0.5827\n",
      "Episode: 306 Total reward: 11.0 Training loss: 24.7255 Explore P: 0.5821\n",
      "Episode: 307 Total reward: 14.0 Training loss: 0.7392 Explore P: 0.5813\n",
      "Episode: 308 Total reward: 30.0 Training loss: 24.0352 Explore P: 0.5796\n",
      "Episode: 309 Total reward: 16.0 Training loss: 0.5824 Explore P: 0.5787\n",
      "Episode: 310 Total reward: 24.0 Training loss: 23.3826 Explore P: 0.5773\n",
      "Episode: 311 Total reward: 36.0 Training loss: 1.0748 Explore P: 0.5753\n",
      "Episode: 312 Total reward: 26.0 Training loss: 19.1252 Explore P: 0.5738\n",
      "Episode: 313 Total reward: 22.0 Training loss: 0.6562 Explore P: 0.5726\n",
      "Episode: 314 Total reward: 13.0 Training loss: 48.5557 Explore P: 0.5718\n",
      "Episode: 315 Total reward: 9.0 Training loss: 22.9223 Explore P: 0.5713\n",
      "Episode: 316 Total reward: 23.0 Training loss: 0.8993 Explore P: 0.5700\n",
      "Episode: 317 Total reward: 85.0 Training loss: 37.1076 Explore P: 0.5653\n",
      "Episode: 318 Total reward: 10.0 Training loss: 19.1131 Explore P: 0.5647\n",
      "Episode: 319 Total reward: 11.0 Training loss: 38.2714 Explore P: 0.5641\n",
      "Episode: 320 Total reward: 20.0 Training loss: 16.4550 Explore P: 0.5630\n",
      "Episode: 321 Total reward: 14.0 Training loss: 17.9956 Explore P: 0.5623\n",
      "Episode: 322 Total reward: 10.0 Training loss: 0.8340 Explore P: 0.5617\n",
      "Episode: 323 Total reward: 34.0 Training loss: 31.4160 Explore P: 0.5598\n",
      "Episode: 324 Total reward: 14.0 Training loss: 13.1364 Explore P: 0.5591\n",
      "Episode: 325 Total reward: 24.0 Training loss: 19.2959 Explore P: 0.5577\n",
      "Episode: 326 Total reward: 58.0 Training loss: 13.4597 Explore P: 0.5546\n",
      "Episode: 327 Total reward: 21.0 Training loss: 15.8233 Explore P: 0.5534\n",
      "Episode: 328 Total reward: 12.0 Training loss: 0.7849 Explore P: 0.5528\n",
      "Episode: 329 Total reward: 11.0 Training loss: 15.7263 Explore P: 0.5522\n",
      "Episode: 330 Total reward: 12.0 Training loss: 16.4981 Explore P: 0.5515\n",
      "Episode: 331 Total reward: 64.0 Training loss: 14.9623 Explore P: 0.5481\n",
      "Episode: 332 Total reward: 12.0 Training loss: 12.5989 Explore P: 0.5474\n",
      "Episode: 333 Total reward: 20.0 Training loss: 1.1752 Explore P: 0.5464\n",
      "Episode: 334 Total reward: 36.0 Training loss: 13.9360 Explore P: 0.5444\n",
      "Episode: 335 Total reward: 16.0 Training loss: 0.7654 Explore P: 0.5436\n",
      "Episode: 336 Total reward: 26.0 Training loss: 27.3177 Explore P: 0.5422\n",
      "Episode: 337 Total reward: 38.0 Training loss: 42.7347 Explore P: 0.5402\n",
      "Episode: 338 Total reward: 30.0 Training loss: 18.6199 Explore P: 0.5386\n",
      "Episode: 339 Total reward: 40.0 Training loss: 14.5795 Explore P: 0.5365\n",
      "Episode: 340 Total reward: 72.0 Training loss: 1.2009 Explore P: 0.5327\n",
      "Episode: 341 Total reward: 41.0 Training loss: 1.1151 Explore P: 0.5306\n",
      "Episode: 342 Total reward: 50.0 Training loss: 0.5853 Explore P: 0.5280\n",
      "Episode: 343 Total reward: 48.0 Training loss: 25.3287 Explore P: 0.5255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 344 Total reward: 27.0 Training loss: 1.2903 Explore P: 0.5241\n",
      "Episode: 345 Total reward: 42.0 Training loss: 1.2634 Explore P: 0.5219\n",
      "Episode: 346 Total reward: 58.0 Training loss: 0.5161 Explore P: 0.5190\n",
      "Episode: 347 Total reward: 41.0 Training loss: 22.3751 Explore P: 0.5169\n",
      "Episode: 348 Total reward: 27.0 Training loss: 1.3643 Explore P: 0.5155\n",
      "Episode: 349 Total reward: 60.0 Training loss: 0.9433 Explore P: 0.5125\n",
      "Episode: 350 Total reward: 26.0 Training loss: 28.9780 Explore P: 0.5112\n",
      "Episode: 351 Total reward: 49.0 Training loss: 1.0038 Explore P: 0.5087\n",
      "Episode: 352 Total reward: 20.0 Training loss: 23.9415 Explore P: 0.5078\n",
      "Episode: 353 Total reward: 33.0 Training loss: 28.5406 Explore P: 0.5061\n",
      "Episode: 354 Total reward: 48.0 Training loss: 35.6093 Explore P: 0.5037\n",
      "Episode: 355 Total reward: 44.0 Training loss: 12.4602 Explore P: 0.5016\n",
      "Episode: 356 Total reward: 30.0 Training loss: 0.6575 Explore P: 0.5001\n",
      "Episode: 357 Total reward: 42.0 Training loss: 1.1176 Explore P: 0.4980\n",
      "Episode: 358 Total reward: 22.0 Training loss: 19.8952 Explore P: 0.4970\n",
      "Episode: 359 Total reward: 27.0 Training loss: 1.0457 Explore P: 0.4957\n",
      "Episode: 360 Total reward: 92.0 Training loss: 0.9879 Explore P: 0.4912\n",
      "Episode: 361 Total reward: 71.0 Training loss: 9.6347 Explore P: 0.4878\n",
      "Episode: 362 Total reward: 54.0 Training loss: 30.9054 Explore P: 0.4852\n",
      "Episode: 363 Total reward: 28.0 Training loss: 10.0902 Explore P: 0.4839\n",
      "Episode: 364 Total reward: 21.0 Training loss: 41.3188 Explore P: 0.4829\n",
      "Episode: 365 Total reward: 21.0 Training loss: 10.2706 Explore P: 0.4819\n",
      "Episode: 366 Total reward: 62.0 Training loss: 15.5144 Explore P: 0.4790\n",
      "Episode: 367 Total reward: 44.0 Training loss: 0.9195 Explore P: 0.4769\n",
      "Episode: 368 Total reward: 42.0 Training loss: 0.8387 Explore P: 0.4750\n",
      "Episode: 369 Total reward: 30.0 Training loss: 24.7190 Explore P: 0.4736\n",
      "Episode: 370 Total reward: 79.0 Training loss: 28.6910 Explore P: 0.4699\n",
      "Episode: 371 Total reward: 22.0 Training loss: 1.2729 Explore P: 0.4689\n",
      "Episode: 372 Total reward: 46.0 Training loss: 15.1222 Explore P: 0.4668\n",
      "Episode: 373 Total reward: 37.0 Training loss: 16.1186 Explore P: 0.4651\n",
      "Episode: 374 Total reward: 36.0 Training loss: 20.5838 Explore P: 0.4635\n",
      "Episode: 375 Total reward: 39.0 Training loss: 0.9021 Explore P: 0.4617\n",
      "Episode: 376 Total reward: 66.0 Training loss: 14.4290 Explore P: 0.4588\n",
      "Episode: 377 Total reward: 25.0 Training loss: 38.9322 Explore P: 0.4576\n",
      "Episode: 378 Total reward: 49.0 Training loss: 25.6147 Explore P: 0.4555\n",
      "Episode: 379 Total reward: 61.0 Training loss: 1.1981 Explore P: 0.4527\n",
      "Episode: 380 Total reward: 108.0 Training loss: 27.4313 Explore P: 0.4480\n",
      "Episode: 381 Total reward: 116.0 Training loss: 2.2992 Explore P: 0.4429\n",
      "Episode: 382 Total reward: 74.0 Training loss: 1.5589 Explore P: 0.4398\n",
      "Episode: 383 Total reward: 47.0 Training loss: 24.5856 Explore P: 0.4377\n",
      "Episode: 384 Total reward: 46.0 Training loss: 13.4826 Explore P: 0.4358\n",
      "Episode: 385 Total reward: 71.0 Training loss: 23.5383 Explore P: 0.4328\n",
      "Episode: 386 Total reward: 62.0 Training loss: 13.7545 Explore P: 0.4301\n",
      "Episode: 387 Total reward: 58.0 Training loss: 66.7147 Explore P: 0.4277\n",
      "Episode: 388 Total reward: 25.0 Training loss: 25.3164 Explore P: 0.4267\n",
      "Episode: 389 Total reward: 42.0 Training loss: 25.5947 Explore P: 0.4249\n",
      "Episode: 390 Total reward: 33.0 Training loss: 12.2929 Explore P: 0.4236\n",
      "Episode: 391 Total reward: 64.0 Training loss: 13.0792 Explore P: 0.4209\n",
      "Episode: 392 Total reward: 40.0 Training loss: 16.5190 Explore P: 0.4193\n",
      "Episode: 393 Total reward: 54.0 Training loss: 1.1038 Explore P: 0.4171\n",
      "Episode: 394 Total reward: 39.0 Training loss: 1.3460 Explore P: 0.4155\n",
      "Episode: 395 Total reward: 105.0 Training loss: 20.5985 Explore P: 0.4113\n",
      "Episode: 396 Total reward: 42.0 Training loss: 67.2741 Explore P: 0.4096\n",
      "Episode: 397 Total reward: 39.0 Training loss: 14.8551 Explore P: 0.4080\n",
      "Episode: 398 Total reward: 11.0 Training loss: 16.7689 Explore P: 0.4076\n",
      "Episode: 399 Total reward: 48.0 Training loss: 11.8363 Explore P: 0.4057\n",
      "Episode: 400 Total reward: 47.0 Training loss: 0.8403 Explore P: 0.4038\n",
      "Episode: 401 Total reward: 44.0 Training loss: 12.8265 Explore P: 0.4021\n",
      "Episode: 402 Total reward: 42.0 Training loss: 13.5252 Explore P: 0.4005\n",
      "Episode: 403 Total reward: 54.0 Training loss: 89.0697 Explore P: 0.3983\n",
      "Episode: 404 Total reward: 66.0 Training loss: 1.2892 Explore P: 0.3958\n",
      "Episode: 405 Total reward: 49.0 Training loss: 42.3955 Explore P: 0.3939\n",
      "Episode: 406 Total reward: 66.0 Training loss: 1.2486 Explore P: 0.3914\n",
      "Episode: 407 Total reward: 74.0 Training loss: 0.8137 Explore P: 0.3886\n",
      "Episode: 408 Total reward: 67.0 Training loss: 1.9241 Explore P: 0.3860\n",
      "Episode: 409 Total reward: 71.0 Training loss: 1.3000 Explore P: 0.3834\n",
      "Episode: 410 Total reward: 51.0 Training loss: 16.8302 Explore P: 0.3815\n",
      "Episode: 411 Total reward: 128.0 Training loss: 21.1425 Explore P: 0.3768\n",
      "Episode: 412 Total reward: 36.0 Training loss: 0.9526 Explore P: 0.3754\n",
      "Episode: 413 Total reward: 63.0 Training loss: 1.9874 Explore P: 0.3731\n",
      "Episode: 414 Total reward: 84.0 Training loss: 3.5736 Explore P: 0.3701\n",
      "Episode: 415 Total reward: 33.0 Training loss: 25.4947 Explore P: 0.3689\n",
      "Episode: 416 Total reward: 100.0 Training loss: 22.1677 Explore P: 0.3654\n",
      "Episode: 417 Total reward: 37.0 Training loss: 1.0711 Explore P: 0.3640\n",
      "Episode: 418 Total reward: 62.0 Training loss: 37.8361 Explore P: 0.3618\n",
      "Episode: 419 Total reward: 105.0 Training loss: 19.2230 Explore P: 0.3582\n",
      "Episode: 420 Total reward: 82.0 Training loss: 30.0709 Explore P: 0.3553\n",
      "Episode: 421 Total reward: 41.0 Training loss: 1.8227 Explore P: 0.3539\n",
      "Episode: 422 Total reward: 156.0 Training loss: 18.5247 Explore P: 0.3486\n",
      "Episode: 423 Total reward: 54.0 Training loss: 1.8881 Explore P: 0.3468\n",
      "Episode: 424 Total reward: 55.0 Training loss: 1.4954 Explore P: 0.3449\n",
      "Episode: 425 Total reward: 21.0 Training loss: 27.0394 Explore P: 0.3442\n",
      "Episode: 426 Total reward: 116.0 Training loss: 3.5379 Explore P: 0.3404\n",
      "Episode: 427 Total reward: 23.0 Training loss: 17.6067 Explore P: 0.3396\n",
      "Episode: 428 Total reward: 22.0 Training loss: 1.2906 Explore P: 0.3389\n",
      "Episode: 429 Total reward: 146.0 Training loss: 9.4122 Explore P: 0.3341\n",
      "Episode: 430 Total reward: 41.0 Training loss: 28.3989 Explore P: 0.3328\n",
      "Episode: 431 Total reward: 60.0 Training loss: 31.4416 Explore P: 0.3309\n",
      "Episode: 432 Total reward: 103.0 Training loss: 1.4859 Explore P: 0.3276\n",
      "Episode: 433 Total reward: 52.0 Training loss: 18.3467 Explore P: 0.3259\n",
      "Episode: 434 Total reward: 116.0 Training loss: 1.2899 Explore P: 0.3223\n",
      "Episode: 435 Total reward: 84.0 Training loss: 35.6532 Explore P: 0.3197\n",
      "Episode: 436 Total reward: 55.0 Training loss: 19.6334 Explore P: 0.3180\n",
      "Episode: 437 Total reward: 54.0 Training loss: 1.5736 Explore P: 0.3163\n",
      "Episode: 438 Total reward: 104.0 Training loss: 10.7125 Explore P: 0.3131\n",
      "Episode: 439 Total reward: 66.0 Training loss: 89.7016 Explore P: 0.3111\n",
      "Episode: 440 Total reward: 112.0 Training loss: 2.2273 Explore P: 0.3078\n",
      "Episode: 441 Total reward: 87.0 Training loss: 1.1431 Explore P: 0.3052\n",
      "Episode: 442 Total reward: 60.0 Training loss: 1.3136 Explore P: 0.3034\n",
      "Episode: 443 Total reward: 57.0 Training loss: 1.6947 Explore P: 0.3018\n",
      "Episode: 444 Total reward: 118.0 Training loss: 56.8264 Explore P: 0.2984\n",
      "Episode: 445 Total reward: 140.0 Training loss: 1.8525 Explore P: 0.2943\n",
      "Episode: 446 Total reward: 74.0 Training loss: 31.8532 Explore P: 0.2923\n",
      "Episode: 447 Total reward: 65.0 Training loss: 1.4370 Explore P: 0.2904\n",
      "Episode: 448 Total reward: 104.0 Training loss: 1.3530 Explore P: 0.2875\n",
      "Episode: 449 Total reward: 76.0 Training loss: 78.0175 Explore P: 0.2854\n",
      "Episode: 450 Total reward: 54.0 Training loss: 1.5734 Explore P: 0.2839\n",
      "Episode: 451 Total reward: 24.0 Training loss: 118.6859 Explore P: 0.2833\n",
      "Episode: 452 Total reward: 143.0 Training loss: 2.1562 Explore P: 0.2794\n",
      "Episode: 453 Total reward: 47.0 Training loss: 2.3359 Explore P: 0.2781\n",
      "Episode: 454 Total reward: 57.0 Training loss: 38.1454 Explore P: 0.2766\n",
      "Episode: 455 Total reward: 51.0 Training loss: 92.2932 Explore P: 0.2753\n",
      "Episode: 456 Total reward: 64.0 Training loss: 3.3721 Explore P: 0.2736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 457 Total reward: 93.0 Training loss: 37.5053 Explore P: 0.2711\n",
      "Episode: 458 Total reward: 78.0 Training loss: 2.4493 Explore P: 0.2691\n",
      "Episode: 459 Total reward: 77.0 Training loss: 77.7589 Explore P: 0.2671\n",
      "Episode: 460 Total reward: 58.0 Training loss: 1.5039 Explore P: 0.2656\n",
      "Episode: 461 Total reward: 81.0 Training loss: 29.0447 Explore P: 0.2636\n",
      "Episode: 462 Total reward: 61.0 Training loss: 1.3115 Explore P: 0.2620\n",
      "Episode: 463 Total reward: 68.0 Training loss: 45.3246 Explore P: 0.2603\n",
      "Episode: 464 Total reward: 109.0 Training loss: 23.4428 Explore P: 0.2576\n",
      "Episode: 465 Total reward: 138.0 Training loss: 2.1970 Explore P: 0.2542\n",
      "Episode: 466 Total reward: 101.0 Training loss: 2.7041 Explore P: 0.2518\n",
      "Episode: 467 Total reward: 105.0 Training loss: 2.1976 Explore P: 0.2492\n",
      "Episode: 468 Total reward: 117.0 Training loss: 1.4758 Explore P: 0.2464\n",
      "Episode: 469 Total reward: 138.0 Training loss: 48.3709 Explore P: 0.2432\n",
      "Episode: 470 Total reward: 168.0 Training loss: 164.9648 Explore P: 0.2393\n",
      "Episode: 472 Total reward: 21.0 Training loss: 1.4776 Explore P: 0.2343\n",
      "Episode: 473 Total reward: 116.0 Training loss: 0.9820 Explore P: 0.2317\n",
      "Episode: 474 Total reward: 118.0 Training loss: 1.7247 Explore P: 0.2291\n",
      "Episode: 477 Total reward: 68.0 Training loss: 168.2435 Explore P: 0.2191\n",
      "Episode: 479 Total reward: 194.0 Training loss: 1.6819 Explore P: 0.2110\n",
      "Episode: 480 Total reward: 160.0 Training loss: 0.6055 Explore P: 0.2078\n",
      "Episode: 481 Total reward: 196.0 Training loss: 1.3211 Explore P: 0.2040\n",
      "Episode: 483 Total reward: 74.0 Training loss: 1.0155 Explore P: 0.1987\n",
      "Episode: 484 Total reward: 164.0 Training loss: 1.1949 Explore P: 0.1957\n",
      "Episode: 486 Total reward: 82.0 Training loss: 64.8943 Explore P: 0.1905\n",
      "Episode: 488 Total reward: 177.0 Training loss: 1.1417 Explore P: 0.1838\n",
      "Episode: 490 Total reward: 114.0 Training loss: 0.8978 Explore P: 0.1785\n",
      "Episode: 492 Total reward: 92.0 Training loss: 1.7253 Explore P: 0.1736\n",
      "Episode: 494 Total reward: 176.0 Training loss: 0.7780 Explore P: 0.1676\n",
      "Episode: 497 Total reward: 99.0 Training loss: 1.0606 Explore P: 0.1599\n",
      "Episode: 499 Total reward: 117.0 Training loss: 53.9226 Explore P: 0.1552\n",
      "Episode: 501 Total reward: 194.0 Training loss: 1.9541 Explore P: 0.1496\n",
      "Episode: 503 Total reward: 87.0 Training loss: 0.8023 Explore P: 0.1457\n",
      "Episode: 505 Total reward: 86.0 Training loss: 0.7202 Explore P: 0.1418\n",
      "Episode: 507 Total reward: 82.0 Training loss: 1.2812 Explore P: 0.1382\n",
      "Episode: 509 Total reward: 133.0 Training loss: 0.5246 Explore P: 0.1340\n",
      "Episode: 511 Total reward: 180.0 Training loss: 1.5035 Explore P: 0.1294\n",
      "Episode: 514 Total reward: 99.0 Training loss: 1.1176 Explore P: 0.1235\n",
      "Episode: 517 Total reward: 54.0 Training loss: 0.5733 Explore P: 0.1185\n",
      "Episode: 519 Total reward: 189.0 Training loss: 0.4187 Explore P: 0.1144\n",
      "Episode: 522 Total reward: 39.0 Training loss: 0.3261 Explore P: 0.1099\n",
      "Episode: 525 Total reward: 99.0 Training loss: 0.3689 Explore P: 0.1050\n",
      "Episode: 528 Total reward: 99.0 Training loss: 0.1883 Explore P: 0.1004\n",
      "Episode: 530 Total reward: 167.0 Training loss: 0.3775 Explore P: 0.0971\n",
      "Episode: 532 Total reward: 196.0 Training loss: 0.3557 Explore P: 0.0938\n",
      "Episode: 535 Total reward: 63.0 Training loss: 0.1756 Explore P: 0.0900\n",
      "Episode: 538 Total reward: 66.0 Training loss: 0.1582 Explore P: 0.0863\n",
      "Episode: 541 Total reward: 3.0 Training loss: 0.0904 Explore P: 0.0833\n",
      "Episode: 543 Total reward: 125.0 Training loss: 0.1245 Explore P: 0.0810\n",
      "Episode: 545 Total reward: 125.0 Training loss: 0.1972 Explore P: 0.0787\n",
      "Episode: 547 Total reward: 160.0 Training loss: 0.1263 Explore P: 0.0763\n",
      "Episode: 550 Total reward: 11.0 Training loss: 0.0729 Explore P: 0.0736\n",
      "Episode: 552 Total reward: 133.0 Training loss: 0.2458 Explore P: 0.0715\n",
      "Episode: 554 Total reward: 179.0 Training loss: 0.0530 Explore P: 0.0692\n",
      "Episode: 557 Total reward: 22.0 Training loss: 0.0770 Explore P: 0.0668\n",
      "Episode: 560 Total reward: 12.0 Training loss: 0.0907 Explore P: 0.0645\n",
      "Episode: 563 Total reward: 52.0 Training loss: 0.1322 Explore P: 0.0621\n",
      "Episode: 566 Total reward: 99.0 Training loss: 0.0975 Explore P: 0.0595\n",
      "Episode: 569 Total reward: 99.0 Training loss: 0.0194 Explore P: 0.0571\n",
      "Episode: 572 Total reward: 99.0 Training loss: 0.2284 Explore P: 0.0548\n",
      "Episode: 575 Total reward: 99.0 Training loss: 0.1973 Explore P: 0.0527\n",
      "Episode: 578 Total reward: 99.0 Training loss: 0.1149 Explore P: 0.0506\n",
      "Episode: 581 Total reward: 99.0 Training loss: 0.1440 Explore P: 0.0486\n",
      "Episode: 584 Total reward: 99.0 Training loss: 0.0492 Explore P: 0.0467\n",
      "Episode: 587 Total reward: 99.0 Training loss: 0.1065 Explore P: 0.0449\n",
      "Episode: 590 Total reward: 99.0 Training loss: 0.0808 Explore P: 0.0432\n",
      "Episode: 593 Total reward: 99.0 Training loss: 0.1260 Explore P: 0.0416\n",
      "Episode: 596 Total reward: 99.0 Training loss: 0.1107 Explore P: 0.0401\n",
      "Episode: 599 Total reward: 99.0 Training loss: 0.1403 Explore P: 0.0386\n",
      "Episode: 602 Total reward: 99.0 Training loss: 0.2904 Explore P: 0.0372\n",
      "Episode: 605 Total reward: 9.0 Training loss: 0.7793 Explore P: 0.0361\n",
      "Episode: 607 Total reward: 93.0 Training loss: 0.8143 Explore P: 0.0354\n",
      "Episode: 609 Total reward: 65.0 Training loss: 0.7254 Explore P: 0.0347\n",
      "Episode: 611 Total reward: 12.0 Training loss: 1.3735 Explore P: 0.0342\n",
      "Episode: 612 Total reward: 10.0 Training loss: 1.3058 Explore P: 0.0342\n",
      "Episode: 613 Total reward: 13.0 Training loss: 1.2293 Explore P: 0.0341\n",
      "Episode: 614 Total reward: 12.0 Training loss: 1.3083 Explore P: 0.0341\n",
      "Episode: 615 Total reward: 9.0 Training loss: 0.9758 Explore P: 0.0341\n",
      "Episode: 616 Total reward: 9.0 Training loss: 0.9435 Explore P: 0.0341\n",
      "Episode: 617 Total reward: 12.0 Training loss: 1.0868 Explore P: 0.0340\n",
      "Episode: 618 Total reward: 8.0 Training loss: 1.3333 Explore P: 0.0340\n",
      "Episode: 619 Total reward: 9.0 Training loss: 1.0359 Explore P: 0.0340\n",
      "Episode: 620 Total reward: 12.0 Training loss: 0.8629 Explore P: 0.0340\n",
      "Episode: 621 Total reward: 12.0 Training loss: 1.1832 Explore P: 0.0339\n",
      "Episode: 622 Total reward: 11.0 Training loss: 2.1501 Explore P: 0.0339\n",
      "Episode: 623 Total reward: 10.0 Training loss: 1083.7562 Explore P: 0.0339\n",
      "Episode: 624 Total reward: 10.0 Training loss: 0.8744 Explore P: 0.0339\n",
      "Episode: 625 Total reward: 9.0 Training loss: 0.9553 Explore P: 0.0338\n",
      "Episode: 626 Total reward: 13.0 Training loss: 0.8263 Explore P: 0.0338\n",
      "Episode: 627 Total reward: 12.0 Training loss: 1.0143 Explore P: 0.0338\n",
      "Episode: 628 Total reward: 8.0 Training loss: 1.2867 Explore P: 0.0338\n",
      "Episode: 629 Total reward: 11.0 Training loss: 1.5801 Explore P: 0.0337\n",
      "Episode: 630 Total reward: 8.0 Training loss: 2177.5117 Explore P: 0.0337\n",
      "Episode: 631 Total reward: 11.0 Training loss: 0.6356 Explore P: 0.0337\n",
      "Episode: 632 Total reward: 10.0 Training loss: 0.9402 Explore P: 0.0337\n",
      "Episode: 633 Total reward: 13.0 Training loss: 1.4126 Explore P: 0.0336\n",
      "Episode: 634 Total reward: 12.0 Training loss: 0.6577 Explore P: 0.0336\n",
      "Episode: 635 Total reward: 13.0 Training loss: 0.6951 Explore P: 0.0336\n",
      "Episode: 636 Total reward: 16.0 Training loss: 0.9819 Explore P: 0.0335\n",
      "Episode: 637 Total reward: 12.0 Training loss: 1.3685 Explore P: 0.0335\n",
      "Episode: 638 Total reward: 12.0 Training loss: 1390.4896 Explore P: 0.0335\n",
      "Episode: 639 Total reward: 12.0 Training loss: 1.8095 Explore P: 0.0335\n",
      "Episode: 640 Total reward: 16.0 Training loss: 1.6804 Explore P: 0.0334\n",
      "Episode: 641 Total reward: 12.0 Training loss: 2.0468 Explore P: 0.0334\n",
      "Episode: 642 Total reward: 10.0 Training loss: 0.9812 Explore P: 0.0334\n",
      "Episode: 643 Total reward: 15.0 Training loss: 855.1329 Explore P: 0.0333\n",
      "Episode: 646 Total reward: 99.0 Training loss: 1.2804 Explore P: 0.0322\n",
      "Episode: 647 Total reward: 12.0 Training loss: 0.9863 Explore P: 0.0322\n",
      "Episode: 648 Total reward: 12.0 Training loss: 0.8101 Explore P: 0.0321\n",
      "Episode: 649 Total reward: 9.0 Training loss: 1.3854 Explore P: 0.0321\n",
      "Episode: 650 Total reward: 9.0 Training loss: 1.5917 Explore P: 0.0321\n",
      "Episode: 651 Total reward: 10.0 Training loss: 1.3555 Explore P: 0.0321\n",
      "Episode: 652 Total reward: 12.0 Training loss: 0.7326 Explore P: 0.0321\n",
      "Episode: 653 Total reward: 16.0 Training loss: 1.1081 Explore P: 0.0320\n",
      "Episode: 654 Total reward: 12.0 Training loss: 1.3001 Explore P: 0.0320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 655 Total reward: 14.0 Training loss: 1.4822 Explore P: 0.0320\n",
      "Episode: 656 Total reward: 10.0 Training loss: 0.9625 Explore P: 0.0319\n",
      "Episode: 657 Total reward: 10.0 Training loss: 1.3029 Explore P: 0.0319\n",
      "Episode: 658 Total reward: 9.0 Training loss: 1.1219 Explore P: 0.0319\n",
      "Episode: 659 Total reward: 9.0 Training loss: 1.1812 Explore P: 0.0319\n",
      "Episode: 660 Total reward: 8.0 Training loss: 1.6215 Explore P: 0.0319\n",
      "Episode: 661 Total reward: 11.0 Training loss: 1.0508 Explore P: 0.0318\n",
      "Episode: 662 Total reward: 14.0 Training loss: 0.8721 Explore P: 0.0318\n",
      "Episode: 663 Total reward: 10.0 Training loss: 1.7934 Explore P: 0.0318\n",
      "Episode: 664 Total reward: 12.0 Training loss: 1.4585 Explore P: 0.0318\n",
      "Episode: 665 Total reward: 8.0 Training loss: 1.2889 Explore P: 0.0317\n",
      "Episode: 666 Total reward: 9.0 Training loss: 1.2616 Explore P: 0.0317\n",
      "Episode: 667 Total reward: 10.0 Training loss: 3.1264 Explore P: 0.0317\n",
      "Episode: 668 Total reward: 8.0 Training loss: 1.3397 Explore P: 0.0317\n",
      "Episode: 669 Total reward: 10.0 Training loss: 2.2821 Explore P: 0.0317\n",
      "Episode: 670 Total reward: 12.0 Training loss: 869.3655 Explore P: 0.0316\n",
      "Episode: 671 Total reward: 9.0 Training loss: 1.6507 Explore P: 0.0316\n",
      "Episode: 672 Total reward: 9.0 Training loss: 2174.6069 Explore P: 0.0316\n",
      "Episode: 673 Total reward: 8.0 Training loss: 0.8812 Explore P: 0.0316\n",
      "Episode: 674 Total reward: 8.0 Training loss: 0.9343 Explore P: 0.0316\n",
      "Episode: 675 Total reward: 8.0 Training loss: 2.0725 Explore P: 0.0315\n",
      "Episode: 676 Total reward: 9.0 Training loss: 1.7451 Explore P: 0.0315\n",
      "Episode: 677 Total reward: 12.0 Training loss: 1.7404 Explore P: 0.0315\n",
      "Episode: 678 Total reward: 9.0 Training loss: 1.2992 Explore P: 0.0315\n",
      "Episode: 679 Total reward: 10.0 Training loss: 2.4537 Explore P: 0.0315\n",
      "Episode: 680 Total reward: 9.0 Training loss: 1.1846 Explore P: 0.0314\n",
      "Episode: 681 Total reward: 12.0 Training loss: 1.7067 Explore P: 0.0314\n",
      "Episode: 682 Total reward: 10.0 Training loss: 1.4426 Explore P: 0.0314\n",
      "Episode: 683 Total reward: 9.0 Training loss: 1.0256 Explore P: 0.0314\n",
      "Episode: 684 Total reward: 8.0 Training loss: 1.5767 Explore P: 0.0314\n",
      "Episode: 685 Total reward: 9.0 Training loss: 1.1352 Explore P: 0.0313\n",
      "Episode: 686 Total reward: 9.0 Training loss: 1.1623 Explore P: 0.0313\n",
      "Episode: 687 Total reward: 11.0 Training loss: 543.5063 Explore P: 0.0313\n",
      "Episode: 688 Total reward: 11.0 Training loss: 1.3776 Explore P: 0.0313\n",
      "Episode: 689 Total reward: 9.0 Training loss: 3.0980 Explore P: 0.0313\n",
      "Episode: 690 Total reward: 10.0 Training loss: 2.3877 Explore P: 0.0312\n",
      "Episode: 691 Total reward: 9.0 Training loss: 2.6565 Explore P: 0.0312\n",
      "Episode: 692 Total reward: 9.0 Training loss: 3.3189 Explore P: 0.0312\n",
      "Episode: 693 Total reward: 12.0 Training loss: 2690.7566 Explore P: 0.0312\n",
      "Episode: 694 Total reward: 11.0 Training loss: 2.3759 Explore P: 0.0311\n",
      "Episode: 695 Total reward: 12.0 Training loss: 2.1293 Explore P: 0.0311\n",
      "Episode: 696 Total reward: 8.0 Training loss: 1.8149 Explore P: 0.0311\n",
      "Episode: 697 Total reward: 10.0 Training loss: 2.2136 Explore P: 0.0311\n",
      "Episode: 698 Total reward: 12.0 Training loss: 867.8989 Explore P: 0.0311\n",
      "Episode: 699 Total reward: 12.0 Training loss: 950.1396 Explore P: 0.0310\n",
      "Episode: 700 Total reward: 11.0 Training loss: 1.0205 Explore P: 0.0310\n",
      "Episode: 701 Total reward: 12.0 Training loss: 1.7441 Explore P: 0.0310\n",
      "Episode: 702 Total reward: 11.0 Training loss: 1.4406 Explore P: 0.0310\n",
      "Episode: 703 Total reward: 13.0 Training loss: 1.1345 Explore P: 0.0309\n",
      "Episode: 704 Total reward: 10.0 Training loss: 1.0249 Explore P: 0.0309\n",
      "Episode: 705 Total reward: 11.0 Training loss: 2.1122 Explore P: 0.0309\n",
      "Episode: 706 Total reward: 12.0 Training loss: 860.7072 Explore P: 0.0309\n",
      "Episode: 707 Total reward: 11.0 Training loss: 2.0973 Explore P: 0.0308\n",
      "Episode: 708 Total reward: 15.0 Training loss: 891.9979 Explore P: 0.0308\n",
      "Episode: 709 Total reward: 12.0 Training loss: 1.6561 Explore P: 0.0308\n",
      "Episode: 710 Total reward: 14.0 Training loss: 1.2119 Explore P: 0.0308\n",
      "Episode: 711 Total reward: 20.0 Training loss: 0.6117 Explore P: 0.0307\n",
      "Episode: 714 Total reward: 99.0 Training loss: 1.8505 Explore P: 0.0297\n",
      "Episode: 717 Total reward: 99.0 Training loss: 314.9266 Explore P: 0.0287\n",
      "Episode: 720 Total reward: 99.0 Training loss: 2.0809 Explore P: 0.0278\n",
      "Episode: 723 Total reward: 99.0 Training loss: 2.7968 Explore P: 0.0270\n",
      "Episode: 726 Total reward: 99.0 Training loss: 1.3141 Explore P: 0.0261\n",
      "Episode: 729 Total reward: 99.0 Training loss: 260.5697 Explore P: 0.0254\n",
      "Episode: 732 Total reward: 99.0 Training loss: 0.6054 Explore P: 0.0246\n",
      "Episode: 735 Total reward: 99.0 Training loss: 4.7082 Explore P: 0.0239\n",
      "Episode: 738 Total reward: 99.0 Training loss: 3.7998 Explore P: 0.0232\n",
      "Episode: 741 Total reward: 99.0 Training loss: 3.0321 Explore P: 0.0226\n",
      "Episode: 744 Total reward: 99.0 Training loss: 4.7218 Explore P: 0.0220\n",
      "Episode: 747 Total reward: 99.0 Training loss: 43.1445 Explore P: 0.0214\n",
      "Episode: 750 Total reward: 99.0 Training loss: 38.5996 Explore P: 0.0208\n",
      "Episode: 753 Total reward: 99.0 Training loss: 0.8174 Explore P: 0.0203\n",
      "Episode: 756 Total reward: 99.0 Training loss: 0.9088 Explore P: 0.0198\n",
      "Episode: 759 Total reward: 99.0 Training loss: 1.6975 Explore P: 0.0193\n",
      "Episode: 762 Total reward: 99.0 Training loss: 1.3781 Explore P: 0.0189\n",
      "Episode: 765 Total reward: 99.0 Training loss: 2.6268 Explore P: 0.0184\n",
      "Episode: 768 Total reward: 75.0 Training loss: 2.3731 Explore P: 0.0180\n",
      "Episode: 770 Total reward: 151.0 Training loss: 0.6272 Explore P: 0.0178\n",
      "Episode: 773 Total reward: 28.0 Training loss: 0.1973 Explore P: 0.0174\n",
      "Episode: 776 Total reward: 99.0 Training loss: 0.1710 Explore P: 0.0171\n",
      "Episode: 779 Total reward: 99.0 Training loss: 0.2471 Explore P: 0.0167\n",
      "Episode: 782 Total reward: 99.0 Training loss: 0.1157 Explore P: 0.0164\n",
      "Episode: 785 Total reward: 99.0 Training loss: 0.5973 Explore P: 0.0161\n",
      "Episode: 788 Total reward: 99.0 Training loss: 0.0946 Explore P: 0.0158\n",
      "Episode: 791 Total reward: 99.0 Training loss: 0.0664 Explore P: 0.0155\n",
      "Episode: 794 Total reward: 99.0 Training loss: 0.1986 Explore P: 0.0152\n",
      "Episode: 797 Total reward: 99.0 Training loss: 0.2778 Explore P: 0.0150\n",
      "Episode: 800 Total reward: 99.0 Training loss: 0.0775 Explore P: 0.0148\n",
      "Episode: 803 Total reward: 99.0 Training loss: 0.1095 Explore P: 0.0145\n",
      "Episode: 806 Total reward: 99.0 Training loss: 0.1169 Explore P: 0.0143\n",
      "Episode: 809 Total reward: 99.0 Training loss: 0.5996 Explore P: 0.0141\n",
      "Episode: 812 Total reward: 99.0 Training loss: 0.1168 Explore P: 0.0139\n",
      "Episode: 815 Total reward: 99.0 Training loss: 0.1381 Explore P: 0.0137\n",
      "Episode: 818 Total reward: 99.0 Training loss: 0.1152 Explore P: 0.0135\n",
      "Episode: 821 Total reward: 99.0 Training loss: 0.0696 Explore P: 0.0133\n",
      "Episode: 824 Total reward: 99.0 Training loss: 0.0695 Explore P: 0.0132\n",
      "Episode: 827 Total reward: 99.0 Training loss: 0.0564 Explore P: 0.0130\n",
      "Episode: 830 Total reward: 99.0 Training loss: 0.1651 Explore P: 0.0129\n",
      "Episode: 833 Total reward: 99.0 Training loss: 0.0443 Explore P: 0.0127\n",
      "Episode: 836 Total reward: 99.0 Training loss: 0.0850 Explore P: 0.0126\n",
      "Episode: 839 Total reward: 99.0 Training loss: 0.1439 Explore P: 0.0125\n",
      "Episode: 842 Total reward: 99.0 Training loss: 0.0476 Explore P: 0.0124\n",
      "Episode: 845 Total reward: 99.0 Training loss: 340.1510 Explore P: 0.0122\n",
      "Episode: 848 Total reward: 99.0 Training loss: 0.0521 Explore P: 0.0121\n",
      "Episode: 851 Total reward: 99.0 Training loss: 0.0896 Explore P: 0.0120\n",
      "Episode: 854 Total reward: 99.0 Training loss: 0.0339 Explore P: 0.0119\n",
      "Episode: 857 Total reward: 99.0 Training loss: 0.0431 Explore P: 0.0118\n",
      "Episode: 860 Total reward: 99.0 Training loss: 0.0819 Explore P: 0.0118\n",
      "Episode: 863 Total reward: 99.0 Training loss: 0.1286 Explore P: 0.0117\n",
      "Episode: 866 Total reward: 36.0 Training loss: 0.0446 Explore P: 0.0116\n",
      "Episode: 869 Total reward: 99.0 Training loss: 0.0706 Explore P: 0.0115\n",
      "Episode: 872 Total reward: 99.0 Training loss: 0.0566 Explore P: 0.0114\n",
      "Episode: 875 Total reward: 99.0 Training loss: 0.1149 Explore P: 0.0114\n",
      "Episode: 878 Total reward: 99.0 Training loss: 0.0879 Explore P: 0.0113\n",
      "Episode: 881 Total reward: 99.0 Training loss: 0.1588 Explore P: 0.0112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 883 Total reward: 177.0 Training loss: 0.0557 Explore P: 0.0112\n",
      "Episode: 885 Total reward: 140.0 Training loss: 0.0765 Explore P: 0.0112\n",
      "Episode: 887 Total reward: 170.0 Training loss: 0.0433 Explore P: 0.0111\n",
      "Episode: 890 Total reward: 46.0 Training loss: 0.0940 Explore P: 0.0111\n",
      "Episode: 893 Total reward: 99.0 Training loss: 201.9364 Explore P: 0.0110\n",
      "Episode: 896 Total reward: 99.0 Training loss: 0.0413 Explore P: 0.0110\n",
      "Episode: 899 Total reward: 99.0 Training loss: 0.0570 Explore P: 0.0109\n",
      "Episode: 901 Total reward: 133.0 Training loss: 0.2602 Explore P: 0.0109\n",
      "Episode: 904 Total reward: 99.0 Training loss: 0.0477 Explore P: 0.0108\n",
      "Episode: 907 Total reward: 43.0 Training loss: 0.1238 Explore P: 0.0108\n",
      "Episode: 910 Total reward: 99.0 Training loss: 0.1201 Explore P: 0.0108\n",
      "Episode: 912 Total reward: 170.0 Training loss: 0.0808 Explore P: 0.0107\n",
      "Episode: 915 Total reward: 99.0 Training loss: 0.0775 Explore P: 0.0107\n",
      "Episode: 918 Total reward: 99.0 Training loss: 0.0588 Explore P: 0.0107\n",
      "Episode: 921 Total reward: 99.0 Training loss: 0.0622 Explore P: 0.0106\n",
      "Episode: 924 Total reward: 99.0 Training loss: 0.1408 Explore P: 0.0106\n",
      "Episode: 927 Total reward: 99.0 Training loss: 0.0483 Explore P: 0.0106\n",
      "Episode: 930 Total reward: 99.0 Training loss: 0.0894 Explore P: 0.0105\n",
      "Episode: 933 Total reward: 99.0 Training loss: 0.0509 Explore P: 0.0105\n",
      "Episode: 936 Total reward: 99.0 Training loss: 0.0215 Explore P: 0.0105\n",
      "Episode: 939 Total reward: 99.0 Training loss: 0.0467 Explore P: 0.0105\n",
      "Episode: 942 Total reward: 99.0 Training loss: 0.0764 Explore P: 0.0104\n",
      "Episode: 945 Total reward: 99.0 Training loss: 0.0342 Explore P: 0.0104\n",
      "Episode: 948 Total reward: 99.0 Training loss: 0.0477 Explore P: 0.0104\n",
      "Episode: 951 Total reward: 99.0 Training loss: 0.0480 Explore P: 0.0104\n",
      "Episode: 954 Total reward: 99.0 Training loss: 0.0442 Explore P: 0.0104\n",
      "Episode: 957 Total reward: 99.0 Training loss: 0.0310 Explore P: 0.0104\n",
      "Episode: 960 Total reward: 99.0 Training loss: 0.0603 Explore P: 0.0103\n",
      "Episode: 963 Total reward: 99.0 Training loss: 0.1531 Explore P: 0.0103\n",
      "Episode: 966 Total reward: 99.0 Training loss: 0.0247 Explore P: 0.0103\n",
      "Episode: 969 Total reward: 99.0 Training loss: 0.1070 Explore P: 0.0103\n",
      "Episode: 972 Total reward: 99.0 Training loss: 0.3173 Explore P: 0.0103\n",
      "Episode: 975 Total reward: 99.0 Training loss: 0.0780 Explore P: 0.0103\n",
      "Episode: 978 Total reward: 99.0 Training loss: 0.0854 Explore P: 0.0102\n",
      "Episode: 981 Total reward: 99.0 Training loss: 0.0977 Explore P: 0.0102\n",
      "Episode: 984 Total reward: 99.0 Training loss: 0.0460 Explore P: 0.0102\n",
      "Episode: 987 Total reward: 99.0 Training loss: 0.1386 Explore P: 0.0102\n",
      "Episode: 990 Total reward: 99.0 Training loss: 0.1167 Explore P: 0.0102\n",
      "Episode: 993 Total reward: 99.0 Training loss: 0.0626 Explore P: 0.0102\n",
      "Episode: 996 Total reward: 99.0 Training loss: 0.1236 Explore P: 0.0102\n",
      "Episode: 999 Total reward: 99.0 Training loss: 0.0763 Explore P: 0.0102\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    step = 0\n",
    "    for ep in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                action = np.argmax(Qs)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train network\n",
    "            target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            target_Qs[episode_ends] = (0, 0)\n",
    "            \n",
    "            targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "            loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                feed_dict={mainQN.inputs_: states,\n",
    "                                           mainQN.targetQs_: targets,\n",
    "                                           mainQN.actions_: actions})\n",
    "        \n",
    "    saver.save(sess, \"checkpoints/cartpole.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below we plot the total rewards for each episode. The rolling average is plotted in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total Reward')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f870299c668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, rews = np.array(rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_21_1.png)\n",
    "\n",
    "\n",
    "## Playing Atari Games\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
